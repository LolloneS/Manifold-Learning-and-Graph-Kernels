{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Learning and Graph Kernels\n",
    "### Requirements \n",
    "Read [this article](http://www.dsi.unive.it/~atorsell/AI/graph/Unfolding.pdf) presenting a way to improve the disciminative power of graph kernels.\n",
    "Choose one [graph kernel](http://www.dsi.unive.it/~atorsell/AI/graph/kernels.pdf) among\n",
    "\n",
    "* Shortest-path Kernel\n",
    "* Graphlet Kernel\n",
    "* Random Walk Kernel\n",
    "* Weisfeiler-Lehman Kernel\n",
    "\n",
    "\n",
    "Choose one manifold learning technique among\n",
    "\n",
    "* Isomap\n",
    "* Diffusion Maps\n",
    "* Laplacian Eigenmaps\n",
    "* Local Linear Embedding\n",
    "\n",
    "\n",
    "Compare the performance of an SVM trained on the given kernel, with or without the manifold learning step, on the following datasets:\n",
    "\n",
    "* [PPI](http://www.dsi.unive.it/~atorsell/AI/graph/PPI.mat): this is a Protein-Protein Interaction dataset. Here proteins (nodes) are connected by an edge in the graph if they have a physical or functional association.\n",
    "* [Shock](http://www.dsi.unive.it/~atorsell/AI/graph/Shock.mat): representing 2D shapes. Each graph is a skeletal-based representation of the differential structure of the boundary of a 2D shape.\n",
    "\n",
    "**Note**: the datasets are contained in Matlab files. The variable `G` contains a vector of cells, one per graph. \n",
    "The entry `am` of each cell is the adjacency matrix of the graph.\n",
    "The variable `labels`, contains the class-labels of each graph.\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import threading\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from sklearn import svm, manifold, preprocessing\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the datasets\n",
    "* Load the datasets\n",
    "* Take the `G` and `labels` fields\n",
    "* `G` consists in a list containing a list of arrays. Remove the external list. Afterwards take only the `am` field\n",
    "* `labels` consists in many one-element lists. Create a single list removing one depth level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URI = '/home/lorenzo/Dropbox/manifold-learning-and-graph-kernels/dataset/'\n",
    "SHOCK_URI = 'SHOCK.mat'\n",
    "PPI_URI = 'PPI.mat'\n",
    "# `G` and `labels`\n",
    "SHOCK = sio.loadmat(BASE_URI + SHOCK_URI)\n",
    "SHOCK_G = SHOCK['G'][0] # read and get rid of external list\n",
    "# print(SHOCK_G.dtype.names) \n",
    "# [('am', 'O'), ('nl', 'O'), ('al', 'O')]\n",
    "SHOCK_G_adj = SHOCK_G['am'] # take only the adjacency matrix\n",
    "SHOCK_labels = SHOCK['labels'].ravel() # read and get rid of useless 1-element lists\n",
    "assert len(SHOCK_G_adj) == len(SHOCK_labels)\n",
    "del SHOCK\n",
    "PPI = sio.loadmat(BASE_URI + PPI_URI)\n",
    "PPI_G = PPI['G'][0]\n",
    "PPI_G_adj = PPI_G['am']\n",
    "PPI_labels = PPI['labels'].ravel()\n",
    "assert len(PPI_G_adj) == len(PPI_labels)\n",
    "del PPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weisfeiler-Lehman\n",
    "### Algorithm\n",
    "\n",
    "1. Multiset label determination\n",
    "    * assign a multiset label $M_i(v)$ to each node $v \\in G$ which consists of the multiset $\\{l_{i-1}(u)$ | u is a neighbor of v$\\}$\n",
    "        * done in `determine_labels`\n",
    "        * as per the paper, since our graphs are unlabelled, we use the node-degrees as starting labels for the node\n",
    "\n",
    "\n",
    "2. Sorting each multiset\n",
    "    * Sort elements in $M_i(v)$ in ascending order and concatenate them into a string $s_i(v)$\n",
    "        * sorted and merged in `get_labels` \n",
    "    * Add $l_{i−1}(v)$ as a prefix to $s_i(v)$\n",
    "        * done in `extend_labels`. Returns the string formatted as requested\n",
    "\n",
    "\n",
    "3. Label compression\n",
    "    * Map each string $s_i(v)$ to a compressed label using a hash function $f : \\Sigma^∗ \\rightarrow \\Sigma$ such that $f(s_i(v)) = f(s_i (w))$ if and only if $s_i(v) = s_i(w)$\n",
    "        * done in `compress_label` and `relabel`\n",
    "    * As the first \"hash\", I use the highest degree of a node in all graphs, plus one (hence I'm sure that one is a hash instead of an original label\n",
    "\n",
    "\n",
    "4. Relabeling\n",
    "    * Set $l_i(v) = f(s_i(v))$ for all nodes in $G$ \n",
    "        * done in `relabel`\n",
    "\n",
    "\n",
    "After having done all of the above, the similarity matrix for the N graphs is computed.\n",
    "The `run()` method returns the similarity matrix containing the normalized values for all the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeisfeilerLehman:\n",
    "    '''\n",
    "    Get a graph's starting labels (node degrees).\n",
    "    graph --> actual graph\n",
    "    '''\n",
    "    def get_graph_starting_labels(self, graph):\n",
    "        return np.dot(graph, np.ones((len(graph), 1)))\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Get the starting labels for all the graphs (node degrees)\n",
    "    {index of the graph in the graphs list : array representing starting label}\n",
    "    '''\n",
    "    def get_all_starting_labels(self):\n",
    "        starting_labels = {g : self.get_graph_starting_labels(self.graphs[g]) \n",
    "                                                              for g in range(self.n)}\n",
    "        return starting_labels\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Get the highest degree of a node throughout all graphs\n",
    "    '''\n",
    "    def get_max_global_degree(self):\n",
    "        return max([max(v) for _, v in self.get_all_starting_labels().items()])\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Get the neighbors of a node\n",
    "    g --> index of the graph\n",
    "    node --> index of the node\n",
    "    neighbors --> list with indices of the neighbors\n",
    "    '''\n",
    "    def get_neighbors(self, g, node):\n",
    "        graph = self.graphs[g]\n",
    "        neighbors = [j for j in range(len(graph)) if graph[node][j] == 1]\n",
    "        return neighbors\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Get updated labels for a node as per (1)\n",
    "    g --> index of the graph\n",
    "    node --> index of the node in the graph\n",
    "    \n",
    "    Take the labels of the neighbors of a node, sort them, merge them into an unique string\n",
    "    '''\n",
    "    def get_labels(self, g, node):\n",
    "        new_label = sorted([self.labels[g][i] for i in self.get_neighbors(g, node)])\n",
    "        new_label = ''.join(str(int(i)) for i in new_label)\n",
    "        return new_label\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Compute the new multiset of labels of each node in a graph.\n",
    "    Return a dictionary in which the key is the index of the node and the value \n",
    "    is the string returned from the `get_labels` function\n",
    "    g --> index of the graph in the graphs array\n",
    "    '''\n",
    "    def determine_labels(self, g):\n",
    "        new_labels = {k : self.get_labels(g, k) for k in range(len(self.graphs[g]))}\n",
    "        return new_labels\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Return the string obtained from the sorted multiset \n",
    "    g --> index of the graph in the array\n",
    "    '''\n",
    "    def extend_labels(self, g, new_labels):\n",
    "        for k in new_labels: # new_labels is a dict\n",
    "            new_labels[k] = self.labels[g][k] + new_labels[k]\n",
    "        return new_labels    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Compress a label if it has not been compressed already\n",
    "    {long_label : compressed_index}\n",
    "    '''\n",
    "    def compress_label(self, label):\n",
    "        if label not in self.compressed_labels:\n",
    "            self.compressed_labels[label] = str(self.compressed_index)\n",
    "            self.compressed_index += 1\n",
    "        return self.compressed_labels[label]\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Relabel all the nodes in a graph\n",
    "    '''\n",
    "    def relabel(self, g, new_labels):\n",
    "        assert len(new_labels) == len(self.labels[g])\n",
    "        for i in range(len(new_labels)):\n",
    "            self.labels[g][i] = self.compress_label(new_labels[i])\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    Count the original node labels: return a list with the number of occurrences per each label\n",
    "    g --> index of the graph\n",
    "    [0, 1, 2, 3, 1] --> 0 nodes with label 0, 1 node with label 1, ..., 1 node with label 4\n",
    "    '''\n",
    "    def count_original_node_labels(self, g):\n",
    "        phi = []\n",
    "        ol = list(map(int, self.original_labels[g]))\n",
    "        c = Counter(ol)\n",
    "        phi = np.zeros(max(ol)+1)\n",
    "        for k in range(max(ol)+1):\n",
    "            if k in c:\n",
    "                phi[k] = c[k]\n",
    "        return phi\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Count node labels at current iteration: return a list with the number of occurrences per each label\n",
    "    g --> index of the graph\n",
    "    '''\n",
    "    def count_node_labels_at_current_iteration(self, g):\n",
    "        l = list(map(int, [i for _, i in self.compressed_labels.items()]))\n",
    "        c = Counter(self.labels[g])\n",
    "        m = max(int(i) for i in c)\n",
    "        phi = np.zeros(m+1)\n",
    "        for k in range(m+1):\n",
    "            if str(k) in c:\n",
    "                phi[k] = c[str(k)]\n",
    "        return phi\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Prepare feature vectors for the dot product. Make the shorter ones as long as the long ones,\n",
    "    concatenate, and so on.\n",
    "    g1, g2 --> indices of two graphs\n",
    "    '''\n",
    "    def prepare_feature_vectors(self, g1, g2):\n",
    "        l1 = self.count_labels[g1]\n",
    "        l2 = self.count_labels[g2]\n",
    "        tot1 = []\n",
    "        tot2 = []\n",
    "        for i in range(self.h+1):\n",
    "            l = min(len(l1[i]), len(l2[i]))\n",
    "            a1 = l1[i][:l]\n",
    "            a2 = l2[i][:l]\n",
    "            tot1 = np.concatenate((tot1, a1), axis=0)\n",
    "            tot2 = np.concatenate((tot2, a2), axis=0)\n",
    "        return tot1, tot2\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Normalize similarity matrix: sum per rows must be = 1\n",
    "    '''\n",
    "    def normalize_sim_matrix(self):\n",
    "        self.pairwise_sim_matrix = preprocessing.normalize(self.pairwise_sim_matrix)\n",
    "        return self.pairwise_sim_matrix\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Pairwise similarities between all the graphs (unnormalized)\n",
    "    '''\n",
    "    def pairwise_similarities(self):\n",
    "        for i in range(self.n):\n",
    "            for j in range(i, self.n):\n",
    "                g1, g2 = self.prepare_feature_vectors(i, j)\n",
    "                dotp = np.dot(g1, g2)\n",
    "                self.pairwise_sim_matrix[i][j] = dotp\n",
    "                self.pairwise_sim_matrix[j][i] = dotp\n",
    "        self.normalize_sim_matrix()\n",
    "        return self.pairwise_sim_matrix\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Run the whole algorithm: steps 1, 2, 3, 4\n",
    "    '''\n",
    "    def run(self):\n",
    "        for i in range(self.h):\n",
    "            for g in range(self.n): # g is the index of the graph in the array of graphs\n",
    "                new_labels = self.determine_labels(g)\n",
    "                new_labels = self.extend_labels(g, new_labels)\n",
    "                self.relabel(g, new_labels)\n",
    "                self.count_labels[g][i+1] = self.count_node_labels_at_current_iteration(g)\n",
    "        return self.pairwise_similarities()\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Initialize everything\n",
    "    '''\n",
    "    def __init__(self, graphs, h):\n",
    "        self.n = len(graphs)\n",
    "        self.graphs = graphs\n",
    "        self.h = h\n",
    "        self.labels = self.get_all_starting_labels()\n",
    "        self.labels = { \n",
    "                        index : [str(int(degree)) for degree in self.labels[index].ravel()] \n",
    "                                                  for index in self.labels \n",
    "                      }\n",
    "        self.original_labels = deepcopy(self.labels)\n",
    "        self.compressed_index = int(self.get_max_global_degree()[0]) + 1\n",
    "        self.compressed_labels = {}\n",
    "        self.count_labels = {} # {iter1 : {graph1 : count1, graph2 : count2, ...}, ...}\n",
    "        for i in range(self.n):\n",
    "            self.count_labels[i] = {}\n",
    "            self.count_labels[i][0] = self.count_original_node_labels(i)            \n",
    "        del self.original_labels\n",
    "        self.pairwise_sim_matrix = np.zeros((self.n, self.n))\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarity matrices for PPI and SHOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl_PPI = WeisfeilerLehman(PPI_G_adj,4)\n",
    "wl_SHOCK = WeisfeilerLehman(SHOCK_G_adj, 4)\n",
    "\n",
    "t1 = threading.Thread(name=\"PPI\", target=wl_PPI.run)\n",
    "t2 = threading.Thread(name=\"SHOCK\", target=wl_SHOCK.run)\n",
    "\n",
    "threads = [t1, t2]\n",
    "for t in threads:\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "similarity_matrices = [wl_SHOCK.pairwise_sim_matrix, wl_PPI.pairwise_sim_matrix]\n",
    "labels = [SHOCK_labels, PPI_labels]\n",
    "dn = ['SHOCK', 'PPI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the distance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dms = []\n",
    "for i in range(len(similarity_matrices)):\n",
    "    pd = pairwise_distances(similarity_matrices[i], metric='euclidean')\n",
    "    dms.append(pd)\n",
    "    plt.imshow(pd, cmap='Blues')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMs without manifold learning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True) \n",
    "clf = svm.SVC(kernel=\"linear\")\n",
    "for i in range(len(dms)):\n",
    "    cv_score = cross_val_score(clf, dms[i], labels[i], cv = kfold)\n",
    "    print(f'DATASET: {dn[i]}\\nMinimum: {np.min(cv_score)}\\nMean: {np.mean(cv_score)}\\nMax: {np.max(cv_score)}')\n",
    "    print(f'Standard deviation: {np.std(cv_score)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMs with Isomap manifold learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "results_isomap = {}\n",
    "best = [0, 0]\n",
    "worst = [1, 1] \n",
    "\n",
    "results_isomap[dn[0]] = {}\n",
    "results_isomap[dn[1]] = {}\n",
    "\n",
    "\n",
    "for n_neigh in range(2, 25):\n",
    "    for n_comp in range(2, 10):\n",
    "        for i in range(len(dn)):\n",
    "            p = manifold.Isomap(n_neigh, n_comp).fit_transform(dms[i])\n",
    "            cv_score = cross_val_score(clf, p, labels[i], cv = kfold)\n",
    "            if np.mean(cv_score) > best[i]:\n",
    "                results_isomap[dn[i]]['best'] = [{\n",
    "                    'min' : np.min(cv_score),\n",
    "                    'mean' : np.mean(cv_score),\n",
    "                    'max' : np.max(cv_score),\n",
    "                    'stddev' : np.std(cv_score)\n",
    "                }, (n_neigh, n_comp)]\n",
    "                best[i] = np.mean(cv_score)\n",
    "            if np.mean(cv_score) < worst[i]:\n",
    "                results_isomap[dn[i]]['worst'] = [{\n",
    "                    'min' : np.min(cv_score),\n",
    "                    'mean' : np.mean(cv_score),\n",
    "                    'max' : np.max(cv_score),\n",
    "                    'stddev' : np.std(cv_score)\n",
    "                }, (n_neigh, n_comp)]\n",
    "                worst[i] = np.mean(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(results_isomap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMs with LLE manifold learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lle = {}\n",
    "best = [0, 0]\n",
    "worst = [1, 1] \n",
    "\n",
    "results_lle[dn[0]] = {}\n",
    "results_lle[dn[1]] = {}\n",
    "\n",
    "\n",
    "for n_neigh in range(2, 25):\n",
    "    for n_comp in range(2, 10):\n",
    "        for i in range(len(dn)):\n",
    "            p = manifold.LocallyLinearEmbedding(n_neigh, n_comp).fit_transform(dms[i])\n",
    "            cv_score = cross_val_score(clf, p, labels[i], cv = kfold)\n",
    "            if np.mean(cv_score) > best[i]:\n",
    "                results_lle[dn[i]]['best'] = [{\n",
    "                    'min' : np.min(cv_score),\n",
    "                    'mean' : np.mean(cv_score),\n",
    "                    'max' : np.max(cv_score),\n",
    "                    'stddev' : np.std(cv_score)\n",
    "                }, (n_neigh, n_comp)]\n",
    "                best[i] = np.mean(cv_score)\n",
    "            if np.mean(cv_score) < worst[i]:\n",
    "                results_lle[dn[i]]['worst'] = [{\n",
    "                    'min' : np.min(cv_score),\n",
    "                    'mean' : np.mean(cv_score),\n",
    "                    'max' : np.max(cv_score),\n",
    "                    'stddev' : np.std(cv_score)\n",
    "                }, (n_neigh, n_comp)]\n",
    "                worst[i] = np.mean(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(results_lle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
