{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Learning and Graph Kernels\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import sys\n",
    "import threading\n",
    "from copy import deepcopy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the datasets\n",
    "* Load the datasets\n",
    "* Take the `G` and `labels` fields\n",
    "* `G` consists in a list containing a list of arrays. Remove the external list. Afterwards take only the `am` field\n",
    "* `labels` consists in many one-element lists. Create a single list removing one depth level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URI = '/home/lorenzo/Dropbox/manifold-learning-and-graph-kernels/dataset/'\n",
    "SHOCK_URI = 'SHOCK.mat'\n",
    "PPI_URI = 'PPI.mat'\n",
    "# `G` and `labels`\n",
    "SHOCK = sio.loadmat(BASE_URI + SHOCK_URI)\n",
    "SHOCK_G = SHOCK['G'][0] # read and get rid of external list\n",
    "SHOCK_G_adj = SHOCK_G['am'] # take only the adjacency matrix\n",
    "SHOCK_labels = SHOCK['labels'].reshape(-1) # read and get rid of useless 1-element lists\n",
    "assert len(SHOCK_G_adj) == len(SHOCK_labels)\n",
    "del SHOCK\n",
    "PPI = sio.loadmat(BASE_URI + PPI_URI)\n",
    "PPI_G = PPI['G'][0]\n",
    "PPI_G_adj = PPI_G['am']\n",
    "PPI_labels = PPI['labels'].reshape(-1)\n",
    "assert len(PPI_G_adj) == len(PPI_labels)\n",
    "del PPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(SHOCK_G.dtype.names) # [('am', 'O'), ('nl', 'O'), ('al', 'O')]\n",
    "# print(SHOCK_G)\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# x = PPI_G_adj[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = [PPI_G_adj[0]]\n",
    "# print(PPI_G_adj.shape[0])\n",
    "# print(len(PPI_G_adj))\n",
    "# print(np.ones((PPI_G_adj.shape[0],1)))\n",
    "# print(x)\n",
    "# for i in PPI_G_adj:\n",
    "#     print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weisfeiler-Lehman\n",
    "### Algorithm\n",
    "\n",
    "1. Multiset label determination\n",
    "    * assign a multiset label $M_i(v)$ to each node $v \\in G$ which consists of the multiset $\\{l_{i-1}(u)$ | u is a neighbor of v$\\}$\n",
    "        * done in `determine_labels`\n",
    "        * as per the paper, since our graphs are unlabelled, we use the node-degrees as starting labels for the node\n",
    "\n",
    "\n",
    "2. Sorting each multiset\n",
    "    * Sort elements in $M_i(v)$ in ascending order and concatenate them into a string $s_i(v)$\n",
    "        * sorted and merged in `get_labels` \n",
    "    * Add $l_{i−1}(v)$ as a prefix to $s_i(v)$\n",
    "        * done in `get_string_from_multiset`. Returns the string formatted as requested\n",
    "\n",
    "\n",
    "3. Label compression\n",
    "    * Map each string $s_i(v)$ to a compressed label using a hash function $f : \\Sigma^∗ \\rightarrow \\Sigma$ such that $f(s_i(v)) = f(s_i (w))$ if and only if $s_i(v) = s_i(w)$\n",
    "        * done in `compress_label` and `relabel`\n",
    "    * As the first \"hash\", I use the highest degree of a node in all graphs, plus one (hence I'm sure that one is a hash instead of an original label\n",
    "\n",
    "\n",
    "4. Relabeling\n",
    "    * Set $l_i(v) = f(s_i(v))$ for all nodes in $G$ \n",
    "        * done in `relabel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeisfeilerLehman:\n",
    "    '''\n",
    "    Get a graph's starting labels (node degrees).\n",
    "    graph --> actual graph\n",
    "    '''\n",
    "    def get_graph_starting_labels(self, graph):\n",
    "        return np.dot(graph, np.ones((len(graph), 1)))\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Get the starting labels for all the graphs (node degrees)\n",
    "    {index of the graph in the graphs list : array representing starting label}\n",
    "    '''\n",
    "    def get_all_starting_labels(self):\n",
    "        return {g : self.get_graph_starting_labels(self.graphs[g]) for g in range(len(self.graphs))}\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Get the highest degree of a node throughout all graphs\n",
    "    '''\n",
    "    def get_max_global_degree(self):\n",
    "        return max([max(v) for _, v in self.get_all_starting_labels().items()])\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Get the neighbors of a node\n",
    "    g --> index of the graph\n",
    "    node --> index of the node\n",
    "    '''\n",
    "    def get_neighbors(self, g, node):\n",
    "        graph = self.graphs[g]\n",
    "        neighbors = [j for j in range(len(graph)) if graph[node][j] == 1]\n",
    "        return neighbors\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Get updated labels for a node as per (1)\n",
    "    g --> index of the graph\n",
    "    node --> index of the node in the graph\n",
    "    '''\n",
    "    def get_labels(self, g, node):\n",
    "        new_label = sorted([self.labels[g][i] for i in self.get_neighbors(g, node)])\n",
    "        new_label = ''.join(str(int(i)) for i in new_label)\n",
    "        return new_label\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Return the new multiset of labels of each node in a graph\n",
    "    g --> index of the graph in the graphs array\n",
    "    '''\n",
    "    def determine_labels(self, g):\n",
    "        new_labels = {k : self.get_labels(g, k) for k in range(len(self.graphs[g]))}\n",
    "        return new_labels\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Return the string obtained from the sorted multiset \n",
    "    g --> index of the graph in the array\n",
    "    '''\n",
    "    def get_string_from_multiset(self, g, new_labels):\n",
    "        for k in new_labels: # new_labels is a dict\n",
    "            new_labels[k] = self.labels[g][k] + new_labels[k]\n",
    "        return new_labels    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Compress a label if it has not been compressed already\n",
    "    {long_label : compressed_index}\n",
    "    '''\n",
    "    def compress_label(self, label):\n",
    "        if label not in self.compressed_labels:\n",
    "            self.compressed_labels[label] = str(self.compressed_index)\n",
    "            self.compressed_index += 1\n",
    "        return self.compressed_labels[label]\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Relabel all the nodes in a graph\n",
    "    '''\n",
    "    def relabel(self, g, new_labels):\n",
    "        assert len(new_labels) == len(self.labels[g])\n",
    "        for i in range(len(new_labels)):\n",
    "            self.labels[g][i] = self.compress_label(new_labels[i])\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Count the original node labels: return a list with the number of occurrences per each label\n",
    "    g --> index of the graph\n",
    "    [0, 1, 2, 3, 1] --> 0 nodes with label 0, 1 node with label 1, ..., 1 node with label 4\n",
    "    '''\n",
    "    def count_original_node_labels(self, g):\n",
    "        phi = []\n",
    "        ol = list(map(int, self.original_labels[g]))\n",
    "        c = Counter(ol)\n",
    "        for k in range(max(ol)+1):\n",
    "            if k in c:\n",
    "                phi.append(c[k])\n",
    "            else:\n",
    "                phi.append(0)\n",
    "        return phi\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Count node labels at current iteration: return a list with the number of occurrences per each label\n",
    "    g --> index of the graph\n",
    "    '''\n",
    "    def count_node_labels_at_current_iteration(self, g):\n",
    "        phi = []\n",
    "        l = list(map(int, [i for _, i in self.compressed_labels.items()]))\n",
    "        c = Counter(l)\n",
    "        for k in range(max(l)+1):\n",
    "            if k in c:\n",
    "                phi.append(c[k])\n",
    "            else:\n",
    "                phi.append(0)\n",
    "        return phi\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Prepare feature vectors for the dot product. Make the shorter ones as long as the long ones,\n",
    "    concatenate, and so on.\n",
    "    g1, g2 --> indices of two graphs\n",
    "    '''\n",
    "    def prepare_feature_vectors(self, g1, g2):\n",
    "        l1 = self.count_labels[g1]\n",
    "        l2 = self.count_labels[g2]\n",
    "        tot1 = []\n",
    "        tot2 = []\n",
    "        for i in range(self.h+1):\n",
    "            if len(l1[i]) > len(l2[i]):\n",
    "                a1, a2 = np.array(l1[i]), np.zeros(len(l1[i]))\n",
    "                for j in range(len(l2[i])):\n",
    "                    a2[i] = l2[i][j]\n",
    "            else:\n",
    "                a1, a2 = np.zeros(len(l2[i])), np.array(l2[i])\n",
    "                for j in range(len(l1[i])):\n",
    "                    a1[i] = l1[i][j]\n",
    "            tot1 = np.concatenate(tot1, a1)\n",
    "            tot2 = np.concatenate(tot2, a2)\n",
    "        return tot1, tot2\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Normalize similarity matrix: sum per rows must be = 1\n",
    "    '''\n",
    "    def normalize_sim_matrix(self):\n",
    "        for i in self.pairwise_sim_matrix:\n",
    "            s = i.sum()\n",
    "            for j in range(len(i)):\n",
    "                i[j] = i[j]/s\n",
    "        \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Pairwise similarities between all the graphs (unnormalized)\n",
    "    '''\n",
    "    def pairwise_similarities(self):\n",
    "        for i in range(self.n):\n",
    "            for j in range(i, self.n):\n",
    "                g1, g2 = self.prepare_feature_vectors(i, j)\n",
    "                dotp = np.dot(g1, g2)\n",
    "                self.pairwise_sim_matrix[i][j] = dotp\n",
    "                self.pairwise_sim_matrix[j][i] = dotp\n",
    "        self.normalize_sim_matrix()\n",
    "        return self.pairwise_sim_matrix\n",
    "    \n",
    "    '''\n",
    "    Run the whole algorithm: steps 1, 2, 3, 4\n",
    "    '''\n",
    "    def run(self):\n",
    "        for i in range(self.h):\n",
    "            for g in range(self.n): # g is the index of the graph in the array of graphs\n",
    "                new_labels = self.determine_labels(g)\n",
    "                new_labels = self.get_string_from_multiset(g, new_labels)\n",
    "                self.relabel(g, new_labels)\n",
    "                self.count_labels[g][i+1] = self.count_node_labels_at_current_iteration(g)\n",
    "        return self.pairwise_similarities()\n",
    "\n",
    "    '''\n",
    "    Initialize everything and run the algorithm h times\n",
    "    '''\n",
    "    def __init__(self, graphs, h):\n",
    "        self.n = len(graphs)\n",
    "        self.graphs = graphs\n",
    "        self.h = h\n",
    "        self.labels = self.get_all_starting_labels()\n",
    "        self.labels = { \n",
    "                        index : [str(int(degree)) for degree in self.labels[index].ravel()] \n",
    "                                                  for index in self.labels \n",
    "                      }\n",
    "        self.original_labels = deepcopy(self.labels)\n",
    "        self.compressed_index = int(self.get_max_global_degree()[0]) + 1\n",
    "        self.compressed_labels = {}\n",
    "        self.count_labels = {} # {iterazione1 : {grafo1 : conteggio1, grafo2 : conteggio2, ...}, ...}\n",
    "        for i in range(self.n):\n",
    "            self.count_labels[i] = {}\n",
    "            self.count_labels[i][0] = self.count_original_node_labels(i)            \n",
    "        del self.original_labels\n",
    "        self.pairwise_sim_matrix = np.array((self.n, self.n))\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Required argument 'object' (pos 1) not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-2aaaa5d6864a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwl_PPI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWeisfeilerLehman\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPPI_G_adj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwl_PPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-56923db19212>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_node_labels_at_current_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     '''\n",
      "\u001b[0;32m<ipython-input-96-56923db19212>\u001b[0m in \u001b[0;36mpairwise_similarities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_feature_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0mdotp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise_sim_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdotp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-56923db19212>\u001b[0m in \u001b[0;36mprepare_feature_vectors\u001b[0;34m(self, g1, g2)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mtot1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mtot2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Required argument 'object' (pos 1) not found"
     ]
    }
   ],
   "source": [
    "# x = [PPI_G_adj[84]]\n",
    "# print(x)\n",
    "# wl_PPI = WeisfeilerLehman(x, 2)\n",
    "\n",
    "wl_PPI = WeisfeilerLehman(PPI_G_adj,2)\n",
    "\n",
    "wl_PPI.run()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# t1 = threading.Thread(name=\"PPI\", target=wl_PPI.run)\n",
    "# wl_SHOCK = WeisfeilerLehman(SHOCK_G_adj, 2)\n",
    "# t2 = threading.Thread(name=\"SHOCK\", target=wl_SHOCK.run)\n",
    "# threads = [t1, t2]\n",
    "# for t in threads:\n",
    "#     t.start()\n",
    "# for t in threads:\n",
    "#     t.join()\n",
    "# print(wl_PPI.labels)\n",
    "# print(wl_SHOCK.labels)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
